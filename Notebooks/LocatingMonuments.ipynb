{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7963d70",
   "metadata": {},
   "source": [
    "# Leveling Arrays\n",
    "\n",
    "UCSB\n",
    "\n",
    "https://geodesy.projects.geol.ucsb.edu/level_lines/level_regions.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f527ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching main page…\n",
      "Found 63 subpages.\n",
      "Processing: X0004_ANZA.html\n",
      "Processing: X0027_JUNCAL.html\n",
      "Processing: X0031_LEWIS_CREEK.html\n",
      "Processing: X0041_MUSTANG_GRADE.html\n",
      "Processing: X0005_ARTISTS_DRIVE.html\n",
      "Processing: X0008_BIG_TUJUNGA.html\n",
      "Processing: X0016_DAY_CANYON.html\n",
      "Processing: X0030_KOEHN_LAKE.html\n",
      "Processing: X0002_47TH_STREET_EAST.html\n",
      "Processing: X0037_MESA_VALLEY.html\n",
      "Processing: X0064_YMCA.html\n",
      "Processing: X0012_CAMP_DIX.html\n",
      "Processing: X0011_CAMERON.html\n",
      "Processing: X0045_PAINTED_CANYON.html\n",
      "Processing: X0042_NEBO.html\n",
      "Processing: X0054_SB_VALLEY_COLLEGE.html\n",
      "Processing: X0053_SAN_JUAN_BAUTISTA.html\n",
      "Processing: X0025_HECTOR.html\n",
      "Processing: X0050_PORTESUELO.html\n",
      "Processing: X0038_MINA.html\n",
      "Processing: X0058_THOUSAND_PALMS.html\n",
      "Processing: X0026_JPL.html\n",
      "Processing: X0051_SANTA_ANITA_CANYON.html\n",
      "Processing: X0019_FISH_LAKE_VALLEY.html\n",
      "Processing: X0010_CABALLO.html\n",
      "Processing: X0036_McGEE_CREEK.html\n",
      "Processing: X0022_GRAND_TETON.html\n",
      "Processing: X0062_WALLACE_CREEK.html\n",
      "Processing: X0059_TRIANGLE_SPRING.html\n",
      "Processing: X0048_PINYON_FLAT.html\n",
      "Processing: X0052_SAN_FERNANDO.html\n",
      "Processing: X0015_CORVINA_BEACH.html\n",
      "Processing: X0055_SEWAGE_PLANT.html\n",
      "Processing: X0017_DURAVAN.html\n",
      "Processing: X0044_OWENS_RANCH.html\n",
      "Processing: X0043_NORTH_SHORE.html\n",
      "Processing: X0006_BAT_CAVES.html\n",
      "Processing: X0057_SNORT.html\n",
      "Processing: X0007_BIG_ROCK_SPRINGS.html\n",
      "Processing: X0047_PARKFIELD.html\n",
      "Processing: X0039_MIRACLE_HILL.html\n",
      "Processing: X0049_PITT_RANCH.html\n",
      "Processing: X0046_PALLETT_CREEK.html\n",
      "Processing: X0014_COOK_STREET.html\n",
      "Processing: X0013_CAR_HILL.html\n",
      "Processing: X0040_MORONGO_WASH.html\n",
      "Processing: X0009_BOX_CANYON.html\n",
      "Processing: X0029_KERN_FRONT.html\n",
      "Processing: X0003_AIRFIELD.html\n",
      "Processing: X0063_WHITEWATER_CANYON.html\n",
      "Processing: X0032_LITTLEROCK.html\n",
      "Processing: X0024_HANAUPAH_CANYON.html\n",
      "Processing: X0033_LLANO.html\n",
      "Processing: X0056_SILVER_CANYON.html\n",
      "Processing: X0034_LORINDA.html\n",
      "Processing: X0060_TURKEY_FLAT.html\n",
      "Processing: X0035_LOST_LAKE.html\n",
      "Processing: X0028_KANE_SPRINGS.html\n",
      "Processing: X0021_GOLD_HILL.html\n",
      "Processing: X0018_EXTRA.html\n",
      "Processing: X0061_UNA_LAKE.html\n",
      "Processing: X0020_FLENGE_FLAT.html\n",
      "Processing: X0023_GRAPEVINE.html\n",
      "Done! Output written to level_lines_lat_lon_dd.csv\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from html.parser import HTMLParser\n",
    "import html as html_module  # To decode entities like &deg;\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://geodesy.projects.geol.ucsb.edu/level_lines/\"\n",
    "MAIN_PAGE = BASE_URL + \"level_regions.html\"\n",
    "\n",
    "# --- Fetch URL content ---\n",
    "def fetch_url(url):\n",
    "    with urllib.request.urlopen(url) as resp:\n",
    "        return resp.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "# --- Parse subpage links from main page ---\n",
    "class SubpageLinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.links = []\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag.lower() == 'a':\n",
    "            href = dict(attrs).get('href', '')\n",
    "            if href.startswith(\"X\") and href.lower().endswith(\".html\"):\n",
    "                self.links.append(href)\n",
    "\n",
    "def extract_subpage_links(html):\n",
    "    parser = SubpageLinkParser()\n",
    "    parser.feed(html)\n",
    "    return list(set(parser.links))  # Remove duplicates\n",
    "\n",
    "# --- Clean HTML line (strip tags, decode entities) ---\n",
    "class TextExtractor(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.result = []\n",
    "    def handle_data(self, data):\n",
    "        self.result.append(data)\n",
    "    def get_text(self):\n",
    "        return ''.join(self.result)\n",
    "\n",
    "def clean_html_line(line):\n",
    "    parser = TextExtractor()\n",
    "    parser.feed(line)\n",
    "    return html_module.unescape(parser.get_text().strip())\n",
    "\n",
    "# --- DMS to Decimal Degrees ---\n",
    "def dms_to_dd(deg, minutes, seconds, direction=None):\n",
    "    dd = float(deg) + float(minutes)/60 + float(seconds)/3600\n",
    "    if direction and direction.upper() in ['S', 'W']:\n",
    "        dd = -dd\n",
    "    return round(dd, 6)\n",
    "\n",
    "# --- Parse a block of text for DMS coordinates ---\n",
    "def parse_lat_lon_block(text):\n",
    "    lat_dd = lon_dd = None\n",
    "\n",
    "    # Latitude pattern\n",
    "    lat_re = re.compile(r'([NS])?\\s*([0-9]{1,2})°\\s*([0-9]{1,2})\\'\\s*([0-9]{1,2})\"?\\s*([NS])?', re.IGNORECASE)\n",
    "    lon_re = re.compile(r'([EW])?\\s*([0-9]{2,3})°\\s*([0-9]{1,2})\\'\\s*([0-9]{1,2})\"?\\s*([EW])?', re.IGNORECASE)\n",
    "\n",
    "    mlat = lat_re.search(text)\n",
    "    if mlat:\n",
    "        direction = mlat.group(1) or mlat.group(5) or 'N'\n",
    "        lat_dd = dms_to_dd(mlat.group(2), mlat.group(3), mlat.group(4), direction)\n",
    "\n",
    "    mlon = lon_re.search(text)\n",
    "    if mlon:\n",
    "        direction = mlon.group(1) or mlon.group(5) or 'W'\n",
    "        lon_dd = dms_to_dd(mlon.group(2), mlon.group(3), mlon.group(4), direction)\n",
    "\n",
    "    return lat_dd, lon_dd\n",
    "\n",
    "# --- Extract lat/lon from messy HTML ---\n",
    "def extract_lat_lon_from_html(html):\n",
    "    lines = html.splitlines()\n",
    "    lat_dd = lon_dd = None\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = clean_html_line(lines[i])\n",
    "        next_line = clean_html_line(lines[i+1]) if i+1 < len(lines) else \"\"\n",
    "\n",
    "        if 'Latitude' in line and lat_dd is None:\n",
    "            lat_dd, _ = parse_lat_lon_block(line)\n",
    "            if lat_dd is None and next_line:\n",
    "                lat_dd, _ = parse_lat_lon_block(next_line)\n",
    "\n",
    "        if 'Longitude' in line and lon_dd is None:\n",
    "            _, lon_dd = parse_lat_lon_block(line)\n",
    "            if lon_dd is None and next_line:\n",
    "                _, lon_dd = parse_lat_lon_block(next_line)\n",
    "\n",
    "        if lat_dd is not None and lon_dd is not None:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    return lat_dd, lon_dd\n",
    "\n",
    "# --- Main script ---\n",
    "def main():\n",
    "    print(\"Fetching main page…\")\n",
    "    main_html = fetch_url(MAIN_PAGE)\n",
    "    subpages = extract_subpage_links(main_html)\n",
    "    print(f\"Found {len(subpages)} subpages.\")\n",
    "\n",
    "    results = []\n",
    "    for page in subpages:\n",
    "        url = BASE_URL + page\n",
    "        print(f\"Processing: {page}\")\n",
    "        try:\n",
    "            html = fetch_url(url)\n",
    "            lat_dd, lon_dd = extract_lat_lon_from_html(html)\n",
    "            results.append({\n",
    "                \"name\": page,\n",
    "                \"url\": url,\n",
    "                \"latitude_dd\": lat_dd if lat_dd is not None else \"N/A\",\n",
    "                \"longitude_dd\": lon_dd if lon_dd is not None else \"N/A\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {page}: {e}\")\n",
    "        time.sleep(0.2)  # Be nice to the server\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(\"level_lines_lat_lon_dd.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"name\", \"url\", \"latitude_dd\", \"longitude_dd\"])\n",
    "        writer.writeheader()\n",
    "        for row in results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"Done! Output written to level_lines_lat_lon_dd.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0aa020f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GeoJSON written to: level_lines.geojson\n"
     ]
    }
   ],
   "source": [
    "# Convert csv to geojson\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_geojson(csv_path, geojson_path):\n",
    "    features = []\n",
    "\n",
    "    with open(csv_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                lat = float(row[\"latitude_dd\"])\n",
    "                lon = float(row[\"longitude_dd\"])\n",
    "            except (ValueError, KeyError):\n",
    "                continue  # Skip rows with invalid coordinates\n",
    "\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Point\",\n",
    "                    \"coordinates\": [lon, lat],  # GeoJSON uses [lon, lat]\n",
    "                },\n",
    "                \"properties\": {\n",
    "                    \"name\": row.get(\"name\", \"\"),\n",
    "                    \"url\": row.get(\"url\", \"\")\n",
    "                }\n",
    "            }\n",
    "            features.append(feature)\n",
    "\n",
    "    geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": features\n",
    "    }\n",
    "\n",
    "    with open(geojson_path, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(geojson, f, indent=2)\n",
    "\n",
    "    print(f\"✅ GeoJSON written to: {geojson_path}\")\n",
    "\n",
    "# Example usage\n",
    "csv_to_geojson(\"level_lines_lat_lon_dd.csv\", \"level_lines.geojson\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b24fbe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
